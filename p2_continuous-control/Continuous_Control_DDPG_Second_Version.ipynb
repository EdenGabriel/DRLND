{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control Second Version (DDPG)\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use Unity ML-Agents environment for the second project (second version) of the **Deep Reinforcement Learning Nanodegree program**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment\n",
    "\n",
    "Please refer to the file `README.md` for environment preparation.\n",
    "\n",
    "Firstly, we begin by importing the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installation, we begin by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import deque, namedtuple, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from unityagents import UnityEnvironment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will start the environment. \n",
    "\n",
    "Please change the `file_name` parameter to match the location of the Unity Environment that you downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64', no_graphics=True)\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n",
      "(33,) (33,)\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents, env_info.vector_observations.shape[0])\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "print(states[0].shape, states[0,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Agent (using DDPG)\n",
    "Train the agent to solve the environment.\n",
    "\n",
    "The `Agent` contains 4 models: `actor_local`, `actor_target`, `critic_local`, `critic_target`. \n",
    "\n",
    "The **Actor model** structure are as follows:\n",
    "![actor_model](resources/actor_model.png)\n",
    "\n",
    "\n",
    "The **Critic model** structure are as follows:\n",
    "![critic_model](resources/critic_model.png)\n",
    "\n",
    "The following are training codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent\n",
    "\n",
    "# ddpg function for single and multiple agents\n",
    "def ddpg(n_episodes=100,\n",
    "         eps_start=1.0, eps_decay=1e-5, eps_end=0.05,\n",
    "         max_t=10000, learn_every_step=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores, actor_losses, critic_losses = [], [], []\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, 1 + n_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "\n",
    "        agent.reset()\n",
    "\n",
    "        avg_score = 0\n",
    "        actor_loss_list, critic_loss_list = [], []\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states, add_noise=random.random() < eps)\n",
    "            eps = max(eps - eps_decay, eps_end)\n",
    "\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            # Learn, if enough samples are available in memory\n",
    "            if len(agent.memory) > agent.batch_size and \\\n",
    "                    t % learn_every_step == 0:\n",
    "                for _ in range(3):\n",
    "                    experiences = agent.memory.sample(batch_size=agent.batch_size)\n",
    "                    actor_loss, critic_loss = agent.learn(experiences, agent.gamma,\n",
    "                                                          last_action_loss=actor_loss_list[-1] if actor_loss_list else None)\n",
    "                    actor_loss_list.append(actor_loss)\n",
    "                    critic_loss_list.append(critic_loss)\n",
    "\n",
    "            avg_score += np.mean(rewards)\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        scores_deque.append(avg_score)\n",
    "        scores.append(avg_score)\n",
    "        actor_losses.append(np.mean(actor_loss_list))\n",
    "        critic_losses.append(np.mean(critic_loss_list))\n",
    "        print(f\"\\rEpisode {i_episode}\\tExploration: {eps:.6f}\\t\"\n",
    "              f\"Average Score: {np.mean(scores_deque):.2f}\\tCurrent Score: {avg_score:.2f}\\t\"\n",
    "              f\"Actor Loss: {np.mean(actor_loss_list):.2e}\\tCritic Loss: {np.mean(critic_loss_list):.2e}\")\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            # agent.save()\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "        if np.mean(scores_deque) > 30 and len(scores_deque) >= 100:\n",
    "            agent.save()\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            break\n",
    "\n",
    "    return scores, actor_losses, critic_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global parameters\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "ACTOR_LR = 1e-4  # learning rate\n",
    "ACTOR_HIDDEN_UNITS = (256, 128)\n",
    "ACTOR_WEIGHT_DECAY = 1e-5\n",
    "\n",
    "CRITIC_LR = 1e-4  # learning rate\n",
    "CRITIC_HIDDEN_UNITS = (256, 128)\n",
    "CRITIC_WEIGHT_DECAY = 1e-5\n",
    "\n",
    "MEMORY_BUFFER_SIZE = int(1e6)  # maximum size of replay buffer\n",
    "BATCH_SIZE = 128  # mini-batch size\n",
    "GAMMA = 0.99  # discount factor\n",
    "TAU = 1e-3  # interpolation parameter\n",
    "\n",
    "N_EPISODES = 2000  # total episodes to train\n",
    "EPS_START = 1.0  # initial value for exploration (epsilon)\n",
    "EPS_DECAY = 2e-5  # epsilon decay value after each step\n",
    "EPS_END = 0.05  # lower limit of epsilon\n",
    "MAX_STEPS = 10000  # maximum training steps of each epoch\n",
    "LEARN_EVERY_STEP = 5  # extra learning after every step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size,\n",
    "              action_size=action_size,\n",
    "              num_agents=num_agents,\n",
    "              seed=SEED, buffer_size=MEMORY_BUFFER_SIZE,\n",
    "              actor_lr=ACTOR_LR, actor_hidden_sizes=ACTOR_HIDDEN_UNITS, actor_weight_decay=ACTOR_WEIGHT_DECAY,\n",
    "              critic_lr=CRITIC_LR, critic_hidden_sizes=CRITIC_HIDDEN_UNITS, critic_weight_decay=CRITIC_WEIGHT_DECAY,\n",
    "              batch_size=BATCH_SIZE, gamma=GAMMA, tau=TAU)\n",
    "#print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (fc_body): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace)\n",
      "    (4): Linear(in_features=128, out_features=4, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (fc_body): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (critic_body): Sequential(\n",
      "    (0): Linear(in_features=260, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Episode 1\tExploration: 0.979980\tAverage Score: 0.54\tCurrent Score: 0.54\tActor Loss: -9.74e-03\tCritic Loss: 1.68e-05\n",
      "Episode 2\tExploration: 0.959960\tAverage Score: 0.70\tCurrent Score: 0.86\tActor Loss: -1.20e-02\tCritic Loss: 2.52e-05\n",
      "Episode 3\tExploration: 0.939940\tAverage Score: 0.76\tCurrent Score: 0.90\tActor Loss: -1.45e-02\tCritic Loss: 2.93e-05\n",
      "Episode 4\tExploration: 0.919920\tAverage Score: 0.78\tCurrent Score: 0.82\tActor Loss: -1.76e-02\tCritic Loss: 2.94e-05\n",
      "Episode 5\tExploration: 0.899900\tAverage Score: 0.80\tCurrent Score: 0.90\tActor Loss: -2.08e-02\tCritic Loss: 2.85e-05\n",
      "Episode 6\tExploration: 0.879880\tAverage Score: 0.94\tCurrent Score: 1.60\tActor Loss: -2.47e-02\tCritic Loss: 2.98e-05\n",
      "Episode 7\tExploration: 0.859860\tAverage Score: 1.05\tCurrent Score: 1.72\tActor Loss: -2.84e-02\tCritic Loss: 3.26e-05\n",
      "Episode 8\tExploration: 0.839840\tAverage Score: 1.18\tCurrent Score: 2.09\tActor Loss: -3.23e-02\tCritic Loss: 3.62e-05\n",
      "Episode 9\tExploration: 0.819820\tAverage Score: 1.27\tCurrent Score: 2.03\tActor Loss: -3.63e-02\tCritic Loss: 4.34e-05\n",
      "Episode 10\tExploration: 0.799800\tAverage Score: 1.40\tCurrent Score: 2.52\tActor Loss: -4.08e-02\tCritic Loss: 5.43e-05\n",
      "Episode 11\tExploration: 0.779780\tAverage Score: 1.51\tCurrent Score: 2.66\tActor Loss: -4.54e-02\tCritic Loss: 7.03e-05\n",
      "Episode 12\tExploration: 0.759760\tAverage Score: 1.69\tCurrent Score: 3.70\tActor Loss: -5.14e-02\tCritic Loss: 8.89e-05\n",
      "Episode 13\tExploration: 0.739740\tAverage Score: 1.94\tCurrent Score: 4.92\tActor Loss: -5.90e-02\tCritic Loss: 1.12e-04\n",
      "Episode 14\tExploration: 0.719720\tAverage Score: 2.23\tCurrent Score: 5.92\tActor Loss: -6.73e-02\tCritic Loss: 1.41e-04\n",
      "Episode 15\tExploration: 0.699700\tAverage Score: 2.44\tCurrent Score: 5.42\tActor Loss: -7.68e-02\tCritic Loss: 1.91e-04\n",
      "Episode 16\tExploration: 0.679680\tAverage Score: 2.66\tCurrent Score: 5.99\tActor Loss: -8.68e-02\tCritic Loss: 2.18e-04\n",
      "Episode 17\tExploration: 0.659660\tAverage Score: 2.85\tCurrent Score: 5.80\tActor Loss: -9.79e-02\tCritic Loss: 2.74e-04\n",
      "Episode 18\tExploration: 0.639640\tAverage Score: 3.07\tCurrent Score: 6.80\tActor Loss: -1.10e-01\tCritic Loss: 3.35e-04\n",
      "Episode 19\tExploration: 0.619620\tAverage Score: 3.37\tCurrent Score: 8.82\tActor Loss: -1.22e-01\tCritic Loss: 3.97e-04\n",
      "Episode 20\tExploration: 0.599600\tAverage Score: 3.65\tCurrent Score: 8.95\tActor Loss: -1.38e-01\tCritic Loss: 4.73e-04\n",
      "Episode 21\tExploration: 0.579580\tAverage Score: 4.04\tCurrent Score: 11.97\tActor Loss: -1.53e-01\tCritic Loss: 5.49e-04\n",
      "Episode 22\tExploration: 0.559560\tAverage Score: 4.38\tCurrent Score: 11.39\tActor Loss: -1.72e-01\tCritic Loss: 6.27e-04\n",
      "Episode 23\tExploration: 0.539540\tAverage Score: 4.89\tCurrent Score: 16.07\tActor Loss: -1.92e-01\tCritic Loss: 7.13e-04\n",
      "Episode 24\tExploration: 0.519520\tAverage Score: 5.31\tCurrent Score: 15.10\tActor Loss: -2.14e-01\tCritic Loss: 8.40e-04\n",
      "Episode 25\tExploration: 0.499500\tAverage Score: 5.74\tCurrent Score: 16.05\tActor Loss: -2.37e-01\tCritic Loss: 8.95e-04\n",
      "Episode 26\tExploration: 0.479480\tAverage Score: 6.25\tCurrent Score: 19.02\tActor Loss: -2.62e-01\tCritic Loss: 9.92e-04\n",
      "Episode 27\tExploration: 0.459460\tAverage Score: 6.81\tCurrent Score: 21.24\tActor Loss: -2.92e-01\tCritic Loss: 1.14e-03\n",
      "Episode 28\tExploration: 0.439440\tAverage Score: 7.35\tCurrent Score: 21.96\tActor Loss: -3.22e-01\tCritic Loss: 1.20e-03\n",
      "Episode 29\tExploration: 0.419420\tAverage Score: 8.03\tCurrent Score: 27.15\tActor Loss: -3.56e-01\tCritic Loss: 1.26e-03\n",
      "Episode 30\tExploration: 0.399400\tAverage Score: 8.57\tCurrent Score: 24.26\tActor Loss: -3.90e-01\tCritic Loss: 1.36e-03\n",
      "Episode 31\tExploration: 0.379380\tAverage Score: 9.13\tCurrent Score: 25.89\tActor Loss: -4.24e-01\tCritic Loss: 1.58e-03\n",
      "Episode 32\tExploration: 0.359360\tAverage Score: 9.72\tCurrent Score: 27.99\tActor Loss: -4.61e-01\tCritic Loss: 1.55e-03\n",
      "Episode 33\tExploration: 0.339340\tAverage Score: 10.32\tCurrent Score: 29.55\tActor Loss: -5.01e-01\tCritic Loss: 1.68e-03\n",
      "Episode 34\tExploration: 0.319320\tAverage Score: 10.90\tCurrent Score: 30.12\tActor Loss: -5.42e-01\tCritic Loss: 1.83e-03\n",
      "Episode 35\tExploration: 0.299300\tAverage Score: 11.45\tCurrent Score: 30.01\tActor Loss: -5.85e-01\tCritic Loss: 1.87e-03\n",
      "Episode 36\tExploration: 0.279280\tAverage Score: 11.99\tCurrent Score: 30.87\tActor Loss: -6.25e-01\tCritic Loss: 2.00e-03\n",
      "Episode 37\tExploration: 0.259260\tAverage Score: 12.53\tCurrent Score: 32.07\tActor Loss: -6.66e-01\tCritic Loss: 2.01e-03\n",
      "Episode 38\tExploration: 0.239240\tAverage Score: 13.07\tCurrent Score: 33.02\tActor Loss: -7.11e-01\tCritic Loss: 2.23e-03\n",
      "Episode 39\tExploration: 0.219220\tAverage Score: 13.56\tCurrent Score: 32.29\tActor Loss: -7.55e-01\tCritic Loss: 2.30e-03\n",
      "Episode 40\tExploration: 0.199200\tAverage Score: 14.02\tCurrent Score: 32.00\tActor Loss: -7.99e-01\tCritic Loss: 2.53e-03\n",
      "Episode 41\tExploration: 0.179180\tAverage Score: 14.50\tCurrent Score: 33.30\tActor Loss: -8.45e-01\tCritic Loss: 2.46e-03\n",
      "Episode 42\tExploration: 0.159160\tAverage Score: 14.92\tCurrent Score: 32.40\tActor Loss: -8.88e-01\tCritic Loss: 2.83e-03\n",
      "Episode 43\tExploration: 0.139140\tAverage Score: 15.42\tCurrent Score: 36.25\tActor Loss: -9.38e-01\tCritic Loss: 2.86e-03\n",
      "Episode 44\tExploration: 0.119120\tAverage Score: 15.83\tCurrent Score: 33.75\tActor Loss: -9.86e-01\tCritic Loss: 3.12e-03\n",
      "Episode 45\tExploration: 0.099100\tAverage Score: 16.24\tCurrent Score: 34.25\tActor Loss: -1.03e+00\tCritic Loss: 3.41e-03\n",
      "Episode 46\tExploration: 0.079080\tAverage Score: 16.65\tCurrent Score: 34.79\tActor Loss: -1.08e+00\tCritic Loss: 3.40e-03\n",
      "Episode 47\tExploration: 0.059060\tAverage Score: 17.04\tCurrent Score: 35.15\tActor Loss: -1.13e+00\tCritic Loss: 3.32e-03\n",
      "Episode 48\tExploration: 0.050000\tAverage Score: 17.38\tCurrent Score: 33.30\tActor Loss: -1.18e+00\tCritic Loss: 3.71e-03\n",
      "Episode 49\tExploration: 0.050000\tAverage Score: 17.71\tCurrent Score: 33.59\tActor Loss: -1.22e+00\tCritic Loss: 3.67e-03\n",
      "Episode 50\tExploration: 0.050000\tAverage Score: 18.04\tCurrent Score: 34.32\tActor Loss: -1.27e+00\tCritic Loss: 3.84e-03\n",
      "Episode 51\tExploration: 0.050000\tAverage Score: 18.34\tCurrent Score: 33.08\tActor Loss: -1.33e+00\tCritic Loss: 4.44e-03\n",
      "Episode 52\tExploration: 0.050000\tAverage Score: 18.64\tCurrent Score: 34.23\tActor Loss: -1.39e+00\tCritic Loss: 4.31e-03\n",
      "Episode 53\tExploration: 0.050000\tAverage Score: 18.92\tCurrent Score: 33.46\tActor Loss: -1.44e+00\tCritic Loss: 4.80e-03\n",
      "Episode 54\tExploration: 0.050000\tAverage Score: 19.21\tCurrent Score: 34.55\tActor Loss: -1.49e+00\tCritic Loss: 4.23e-03\n",
      "Episode 55\tExploration: 0.050000\tAverage Score: 19.49\tCurrent Score: 34.46\tActor Loss: -1.55e+00\tCritic Loss: 5.17e-03\n",
      "Episode 56\tExploration: 0.050000\tAverage Score: 19.74\tCurrent Score: 33.37\tActor Loss: -1.61e+00\tCritic Loss: 5.55e-03\n",
      "Episode 57\tExploration: 0.050000\tAverage Score: 19.98\tCurrent Score: 33.69\tActor Loss: -1.65e+00\tCritic Loss: 5.29e-03\n",
      "Episode 58\tExploration: 0.050000\tAverage Score: 20.25\tCurrent Score: 35.38\tActor Loss: -1.71e+00\tCritic Loss: 5.72e-03\n",
      "Episode 59\tExploration: 0.050000\tAverage Score: 20.50\tCurrent Score: 35.07\tActor Loss: -1.76e+00\tCritic Loss: 5.37e-03\n",
      "Episode 60\tExploration: 0.050000\tAverage Score: 20.75\tCurrent Score: 35.52\tActor Loss: -1.81e+00\tCritic Loss: 5.15e-03\n",
      "Episode 61\tExploration: 0.050000\tAverage Score: 21.00\tCurrent Score: 36.21\tActor Loss: -1.87e+00\tCritic Loss: 6.03e-03\n",
      "Episode 62\tExploration: 0.050000\tAverage Score: 21.23\tCurrent Score: 35.11\tActor Loss: -1.91e+00\tCritic Loss: 6.42e-03\n",
      "Episode 63\tExploration: 0.050000\tAverage Score: 21.45\tCurrent Score: 35.06\tActor Loss: -1.97e+00\tCritic Loss: 5.74e-03\n",
      "Episode 64\tExploration: 0.050000\tAverage Score: 21.65\tCurrent Score: 34.42\tActor Loss: -2.01e+00\tCritic Loss: 5.94e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 65\tExploration: 0.050000\tAverage Score: 21.85\tCurrent Score: 34.27\tActor Loss: -2.06e+00\tCritic Loss: 6.07e-03\n",
      "Episode 66\tExploration: 0.050000\tAverage Score: 22.06\tCurrent Score: 35.94\tActor Loss: -2.11e+00\tCritic Loss: 5.63e-03\n",
      "Episode 67\tExploration: 0.050000\tAverage Score: 22.28\tCurrent Score: 36.63\tActor Loss: -2.15e+00\tCritic Loss: 6.14e-03\n",
      "Episode 68\tExploration: 0.050000\tAverage Score: 22.48\tCurrent Score: 36.01\tActor Loss: -2.20e+00\tCritic Loss: 5.67e-03\n",
      "Episode 69\tExploration: 0.050000\tAverage Score: 22.68\tCurrent Score: 36.51\tActor Loss: -2.24e+00\tCritic Loss: 6.80e-03\n",
      "Episode 70\tExploration: 0.050000\tAverage Score: 22.87\tCurrent Score: 36.16\tActor Loss: -2.29e+00\tCritic Loss: 6.66e-03\n",
      "Episode 71\tExploration: 0.050000\tAverage Score: 23.06\tCurrent Score: 36.37\tActor Loss: -2.33e+00\tCritic Loss: 6.52e-03\n",
      "Episode 72\tExploration: 0.050000\tAverage Score: 23.23\tCurrent Score: 34.92\tActor Loss: -2.37e+00\tCritic Loss: 6.71e-03\n",
      "Episode 73\tExploration: 0.050000\tAverage Score: 23.40\tCurrent Score: 35.71\tActor Loss: -2.42e+00\tCritic Loss: 6.38e-03\n",
      "Episode 74\tExploration: 0.050000\tAverage Score: 23.56\tCurrent Score: 35.10\tActor Loss: -2.45e+00\tCritic Loss: 7.18e-03\n",
      "Episode 75\tExploration: 0.050000\tAverage Score: 23.73\tCurrent Score: 36.39\tActor Loss: -2.49e+00\tCritic Loss: 6.92e-03\n",
      "Episode 76\tExploration: 0.050000\tAverage Score: 23.88\tCurrent Score: 35.51\tActor Loss: -2.53e+00\tCritic Loss: 8.28e-03\n",
      "Episode 77\tExploration: 0.050000\tAverage Score: 24.04\tCurrent Score: 35.73\tActor Loss: -2.56e+00\tCritic Loss: 7.28e-03\n",
      "Episode 78\tExploration: 0.050000\tAverage Score: 24.19\tCurrent Score: 36.01\tActor Loss: -2.60e+00\tCritic Loss: 7.62e-03\n",
      "Episode 79\tExploration: 0.050000\tAverage Score: 24.33\tCurrent Score: 35.31\tActor Loss: -2.63e+00\tCritic Loss: 8.20e-03\n",
      "Episode 80\tExploration: 0.050000\tAverage Score: 24.47\tCurrent Score: 35.25\tActor Loss: -2.67e+00\tCritic Loss: 9.98e-03\n",
      "Episode 81\tExploration: 0.050000\tAverage Score: 24.61\tCurrent Score: 35.61\tActor Loss: -2.69e+00\tCritic Loss: 6.50e-03\n",
      "Episode 82\tExploration: 0.050000\tAverage Score: 24.74\tCurrent Score: 35.79\tActor Loss: -2.73e+00\tCritic Loss: 8.39e-03\n",
      "Episode 83\tExploration: 0.050000\tAverage Score: 24.85\tCurrent Score: 33.46\tActor Loss: -2.75e+00\tCritic Loss: 9.70e-03\n",
      "Episode 84\tExploration: 0.050000\tAverage Score: 24.97\tCurrent Score: 35.13\tActor Loss: -2.78e+00\tCritic Loss: 8.68e-03\n",
      "Episode 85\tExploration: 0.050000\tAverage Score: 25.08\tCurrent Score: 34.35\tActor Loss: -2.81e+00\tCritic Loss: 9.32e-03\n",
      "Episode 86\tExploration: 0.050000\tAverage Score: 25.20\tCurrent Score: 35.57\tActor Loss: -2.83e+00\tCritic Loss: 7.60e-03\n",
      "Episode 87\tExploration: 0.050000\tAverage Score: 25.29\tCurrent Score: 32.88\tActor Loss: -2.86e+00\tCritic Loss: 9.57e-03\n",
      "Episode 88\tExploration: 0.050000\tAverage Score: 25.40\tCurrent Score: 34.84\tActor Loss: -2.88e+00\tCritic Loss: 9.86e-03\n",
      "Episode 89\tExploration: 0.050000\tAverage Score: 25.50\tCurrent Score: 34.18\tActor Loss: -2.91e+00\tCritic Loss: 8.48e-03\n",
      "Episode 90\tExploration: 0.050000\tAverage Score: 25.59\tCurrent Score: 33.51\tActor Loss: -2.93e+00\tCritic Loss: 9.91e-03\n",
      "Episode 91\tExploration: 0.050000\tAverage Score: 25.68\tCurrent Score: 33.82\tActor Loss: -2.96e+00\tCritic Loss: 8.82e-03\n",
      "Episode 92\tExploration: 0.050000\tAverage Score: 25.78\tCurrent Score: 35.54\tActor Loss: -2.98e+00\tCritic Loss: 1.01e-02\n",
      "Episode 93\tExploration: 0.050000\tAverage Score: 25.88\tCurrent Score: 34.22\tActor Loss: -3.00e+00\tCritic Loss: 9.71e-03\n",
      "Episode 94\tExploration: 0.050000\tAverage Score: 25.98\tCurrent Score: 35.50\tActor Loss: -3.03e+00\tCritic Loss: 8.68e-03\n",
      "Episode 95\tExploration: 0.050000\tAverage Score: 26.07\tCurrent Score: 35.19\tActor Loss: -3.05e+00\tCritic Loss: 1.03e-02\n",
      "Episode 96\tExploration: 0.050000\tAverage Score: 26.17\tCurrent Score: 34.81\tActor Loss: -3.07e+00\tCritic Loss: 1.24e-02\n",
      "Episode 97\tExploration: 0.050000\tAverage Score: 26.26\tCurrent Score: 35.15\tActor Loss: -3.09e+00\tCritic Loss: 1.05e-02\n",
      "Episode 98\tExploration: 0.050000\tAverage Score: 26.36\tCurrent Score: 36.55\tActor Loss: -3.11e+00\tCritic Loss: 1.22e-02\n",
      "Episode 99\tExploration: 0.050000\tAverage Score: 26.46\tCurrent Score: 35.90\tActor Loss: -3.13e+00\tCritic Loss: 1.28e-02\n",
      "Episode 100\tExploration: 0.050000\tAverage Score: 26.55\tCurrent Score: 35.24\tActor Loss: -3.15e+00\tCritic Loss: 1.06e-02\n",
      "Episode 100\tAverage Score: 26.55\n",
      "Episode 101\tExploration: 0.050000\tAverage Score: 26.91\tCurrent Score: 36.27\tActor Loss: -3.17e+00\tCritic Loss: 9.34e-03\n",
      "Episode 102\tExploration: 0.050000\tAverage Score: 27.25\tCurrent Score: 35.12\tActor Loss: -3.20e+00\tCritic Loss: 1.08e-02\n",
      "Episode 103\tExploration: 0.050000\tAverage Score: 27.59\tCurrent Score: 35.17\tActor Loss: -3.21e+00\tCritic Loss: 1.17e-02\n",
      "Episode 104\tExploration: 0.050000\tAverage Score: 27.95\tCurrent Score: 36.29\tActor Loss: -3.23e+00\tCritic Loss: 1.25e-02\n",
      "Episode 105\tExploration: 0.050000\tAverage Score: 28.29\tCurrent Score: 35.62\tActor Loss: -3.25e+00\tCritic Loss: 1.20e-02\n",
      "Episode 106\tExploration: 0.050000\tAverage Score: 28.64\tCurrent Score: 36.30\tActor Loss: -3.27e+00\tCritic Loss: 1.29e-02\n",
      "Episode 107\tExploration: 0.050000\tAverage Score: 28.98\tCurrent Score: 35.88\tActor Loss: -3.29e+00\tCritic Loss: 1.09e-02\n",
      "Episode 108\tExploration: 0.050000\tAverage Score: 29.31\tCurrent Score: 35.28\tActor Loss: -3.31e+00\tCritic Loss: 1.19e-02\n",
      "Episode 109\tExploration: 0.050000\tAverage Score: 29.66\tCurrent Score: 36.87\tActor Loss: -3.33e+00\tCritic Loss: 1.22e-02\n",
      "Episode 110\tExploration: 0.050000\tAverage Score: 29.99\tCurrent Score: 35.12\tActor Loss: -3.35e+00\tCritic Loss: 1.14e-02\n",
      "Episode 111\tExploration: 0.050000\tAverage Score: 30.33\tCurrent Score: 37.33\tActor Loss: -3.36e+00\tCritic Loss: 1.24e-02\n",
      "Episode 111\tAverage Score: 30.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd35346240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(agent)\n",
    "\n",
    "scores, actor_losses, critic_losses = ddpg(n_episodes=N_EPISODES,\n",
    "                                           eps_start=EPS_START, eps_decay=EPS_DECAY, eps_end=EPS_END,\n",
    "                                           max_t=MAX_STEPS, learn_every_step=LEARN_EVERY_STEP)\n",
    "\n",
    "agent.save()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax1.plot(np.arange(1, len(scores) + 1), scores)\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_xlabel('Episode #')\n",
    "\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax2.plot(np.arange(1, len(actor_losses) + 1), actor_losses)\n",
    "# ax2.legend()\n",
    "ax2.set_ylabel('Actor Loss')\n",
    "ax2.set_xlabel('Episode #')\n",
    "\n",
    "ax3 = fig.add_subplot(313)\n",
    "ax3.plot(np.arange(1, len(critic_losses) + 1), critic_losses)\n",
    "ax3.set_ylabel('Critic Loss')\n",
    "ax3.set_xlabel('Episode #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the values from the log information above and rebuild the training plot as follows. (Refer to the file `multi_robots_plot.py`)\n",
    "\n",
    "![training_img](resources/Continuous_Control_DDPG_20_robots.png)\n",
    "\n",
    "From the plot above, the episode is **111** when average score of last 100 episodes larger than 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch the smart Agent to play\n",
    "\n",
    "We load the actor model's parameters to watch the smart agent.\n",
    "\n",
    "**Attention:** The pretrained checkpoints are on GPU machine. There will be an error if you load the checkpoints on non GPU machines, you should do some small changes in Agent#load method in `ddpg_agent.py`. Please refer to [this link](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for hadling the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps: 1001, Total Rewards: 37.376999164559294\n"
     ]
    }
   ],
   "source": [
    "agent.load()\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "\n",
    "t_step, total_rewards = 0, 0\n",
    "while True:\n",
    "    actions = agent.act(states, add_noise=False)\n",
    "\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "\n",
    "    states = next_states\n",
    "    t_step += 1\n",
    "    total_rewards += np.mean(rewards)\n",
    "    if np.any(dones):\n",
    "        break\n",
    "\n",
    "print(f\"Total Steps: {t_step}, Total Rewards: {total_rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Close the Unity Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
